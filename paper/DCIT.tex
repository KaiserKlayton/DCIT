\documentclass[11pt]{article}
%\usepackage{fullpage}

\def\mono#1{\texttt{#1}}  % "mono" command for monospace font

\title{\textbf{DCIT: Discourse Connectives in Twitter}}
\author{Jessica Grasso \mono{jgrasso@uni-potsdam.de}
\\C. Clayton Violand  \mono{charles.violand@uni-potsdam.de}}
\date{21 August 2015}
\begin{document}

\maketitle

\abstract{DCIT is a tool written in Python that analyzes the usage of discourse connectives in German Twitter data. It utilizes Twitter text data, parallel POS-tagged text, and the DiMLex discourse connective framework. It allows for the analysis of the use of discourse connectives amongst Twitter speech, and provides the groundwork for further analysis of this type of dialogue as compared to more standard forms of conversation.}

\section{Background and Related Work}

Discourse markers or discourse connectives are words that show the presence of a discourse relation in text.  These words form a closed class, must meet certain fixed criteria, and share some characteristics of content words.  These relations are vital to the coherence of a text, and can be used to analyze content and structure \cite{dimlex}. \\

Due to the nature of the Twitter platform, one might expect that the use of these discourse connectives differs when tweeting. In order to test this idea, discourse connective data must first be collected from the tweets. To accomplish this, classic data-comparison techniques are brought in, such as POS-tagging.\\

Part-of-speech tagging for Twitter data faces many challenges due to the non-standard language common on that platform, including acronyms, missing or shortened words, emoticons, and URLs, in addition to items such as proper names. \cite{GermanTweetTagging}.  We cannot comment on which, if any, techniques were used to overcome these challenges, as the output files were provided to us, but this is a limitation to keep in mind.\\

This project uses DiMLex, a lexicon of German discourse markers developed by the Discourse Lab at the University of Potsdam \cite{dimlex}.  From this lexicon we extracted our list of German discourse connectives and additional information about each.\\

We also used results found in the bachelor thesis of Angela Schneider (University of Potsdam), titled \emph{Disambiguierung von Diskurskonnektoren im Deutschen} \cite{schneider1}, as well as a paper published by Schneider and Manfred Stede containing a summery of the same work \cite{schneider2}.  The results of these works were used as a first step in disambiguating the potentially ambiguous discourse connectives in this project.

\section{System Description}
DCIT is a tool written in Python that analyzes the usage of discourse connectives in German Twitter data.  Given a list of German discourse markers and one or more files containing German-language tweets, the tool counts possible discourse connectives, performs disambiguation on the ambiguous connectives, and re-counts, outputting annotated versions of the tweets.

\subsection{Input}
\begin{itemize} 

\item \mono{dimlex.xml} - a lexicon of German discourse markers \cite{dimlex}.  At the time of submission the current version was that of 25 July 2015.

\item One or more \mono{.xml} files containing tweet threads.  This format was provided to us and was not altered for this project.  Although not perfect, this data was already cleaned and assumed to contain only German-language tweets from a one-month time period.

\item For each file above, a corresponding \mono{.txt} file containing the part-of-speech tagged text of each tweet and the unique ID number of each tweet for identification purposes.  For this project, these files were provided to us by Wladimir Sidorenko \cite{WladimirSidorenko}, and are tagged using the STTS tag set \cite{stts}.
\end{itemize}

\subsection{Output}
\begin{itemize} 

\item for each tweet thread file, a modified version of that file with the following tags added:
\begin{itemize} 

\item  \mono{has\textunderscore dc} - with value \mono{True} or \mono{False} corresponding to whether the tweet contains at least one (potential) discourse connective
\item  \mono{num\textunderscore dcs} - the number of (potential) discourse connectives contained in that tweet
\item  \mono{dc\textunderscore location\textunderscore x} - for each (potential) discourse connective present, the string index at which it begins in the text.

\end{itemize} % end tags

\end{itemize} % end output

\section{System Method}

First, the DiMLex lexicon is read in and stored as an object (\mono{get\textunderscore dcons.py}).  Each connective has several important features used in this project:
\begin{itemize}
\renewcommand\labelitemi{--}
\item continuity - whether the discourse connective has one part (\emph{unterdessen}, \emph{und zwar}) or multiple parts (\emph{um ... willen}, \emph{umso mehr ... als})
\item type - for each part, whether that part is single (\emph{um}, \emph{seitdem}) or phrasal (\emph{umso mehr}, \emph{und zwar})
\item ambiguity - true for the entries that have multiple possible interpretations, either as a discourse connective or  another function
\end{itemize}
 The lexicon also contains several additional features that we did not utilize. \\
 
Then, the tweets are read in (\mono{get\textunderscore tweets.py}).  Since even one day of tweets is far too much to fit into memory (at least on the machines at our disposal), each tweet is sent through the following pipeline individually, and at no point should more than one file (corresponding to one day) be open simultaneously.  Each tweet is represented as an object, which at this point contains only features extracted from the file.  These include:
\begin{itemize}
\renewcommand\labelitemi{--}
\item \mono{id} - unique tweet ID number
\item \mono{\textunderscore original} - original, unmodified tweet text
\item \mono{words} - tokenized tweet text
\item \mono{raw} - tweet text made lowercase and later further modified as needed (e.g. discourse connective deletion occurs to prevent certain special types from being found twice).
\end{itemize} 

The next step is to determine and save more details about each tweet (\mono{get\textunderscore matches.py}).  This includes searching for all possible discourse connectives, and counting and storing some basic information about how many of each has been found.  This information, since it is collected over a large number of tweets, is stored in another object (\mono{get\textunderscore info.py}) which can be referenced and output at a later time.  In addition to these counts, a list of tuples containing all found (potential) discourse connectives, their ambiguity status, and their location in the string is populated in each tweet object for later use.\\

Once this basic information has been gathered, the task is to disambiguate the ambiguous discourse connectives, that is, to determine which are truly discourse connectives and which are likely to be performing other functions in the text (\mono{disambiguate.py}).  For this task we used Schneider's results, resulting in three categories.  Category 0 includes the words from Table 1 of Schneider \& Stede (2012) \cite{schneider2}, and entirely eliminates words that occur only very infrequently as discourse connectives from the list of possible connectives.  Category 1 includes the words from Table 2.2 of Schneider \cite{schneider1} for which the ambiguous status can be resolved using the part of speech of the connective in question.  Category 2 includes the words from Table 3.1 of Schneider \cite{schneider1} for which further context--in addition to the part-of-speech--is needed in order to disambiguate. \\

After this step, the basic statistical analysis mentioned above is repeated (\mono{post\textunderscore disambiguation\textunderscore stats.py}) using a much simplified but otherwise similar method as before.  Compared with the initial stats, there should now be both fewer potential discourse connectives (as some will have been removed during disambiguation) as well as fewer ambiguous matches.\\
\\
Finally, these results are written to file (\mono{write\textunderscore results.py}).  Currently, the initial files are edited, in that the tags described above are added.  These resulting files can then be used in further analysis.

\section{Results}

These are preliminary results for a subset of the data, namely one day of tweets from the month of April (roughly 1/12th of our full data set). \\

TODOTODOTODOTODOTODOTODO

Pre-disambiguation\\

Post-disambiguation\\

\section{Known Problems and Future Work}
\begin{itemize}
\item \textbf{Optimization} - the tool is quite slow, and could likely be improved through optimization or potentially the incorporation of existing tools or technologies. Using the search and findall functions from the BeautifulSoup package is apparantly expensive.
\item \textbf{Testing} - Due to memory and data constraints, we were unable to perform very thorough testing.  Given the variety of connectives and diversity of Twitter data, this should be improved step.
\item \textbf{Additional disambiguation} - Schneider's work includes results for only a small fraction of the discourse markers included in DiMLex.  While this includes many of the common discourse connectives, other ambiguous connectives remain.  Additional research or other methods could be used to improve disambiguation.
\item \textbf{Conversations} - The project currently focuses on single tweets, not pairs or conversations among Twitter users.  Since tweets are very short texts, and many discourse connectives allow arguments over larger spans, this should be further expanded.

\end{itemize}

\subsection*{Acknowledgements}

Thanks to Paul Ebermann for some suggestions on optimization and Wladamir Sidorenko for providing us with parallel POS-annotated tweet files.

\begin{thebibliography}{} 

\bibitem{schneider1} Angela Schneider.  \emph{Disambiguierung von Diskurskonnektoren im Deutschen} (Unpublished bachelor's thesis). Universit{\"a}t Potsdam.

\bibitem{schneider2} Angela Schneider \& Manfred Stede.  (2012). \emph{Ambiguity in German connectives: A corpus study.}  In: \emph{Proceedings of KONVENS 2012 (Main track: poster presentations)}, 254-258.

\bibitem{stts} Anne Schiller, Simone Teufel, Christine St{\"o}ckelt \& Christine Thielen. (1999).  \emph{Guidelines f{\"u}r das Tagung deutscher Textcorpora mit STTS (Kleines und gro{\ss}es Tagset).}

\bibitem{dimlex}Manfred Stede. (2002). \emph{DiMLex: A lexical approach to discourse markers}. In: A. Lenci, V. Di Tomaso (eds.): \emph{Exploring the Lexicon - Theory and Computation}. Alessandria (Italy): Edizioni dell'Orso, 2002.

\bibitem{GermanTweetTagging} Uladzimir Sidarenka, Tatjana Scheffler, \& Manfred Stede.  (2013).  \emph{Rule based normalization of German Twitter messages}. In: \emph{Proceedings of the International Conference of the German Society for Computational Linguistics and Language Technology}.

\bibitem{WladimirSidorenko} Wladimir Sidorenko.  (20 August 2015).  University of Potsdam staff home page.  \mono{http://www.ling.uni-potsdam.de/staff/sidorenko}

\end{thebibliography}

\end{document}
